{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pythia 70M - check SAEs from Sam Marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linearization.analyzer import SAELinearizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Moving model to device:  cuda\n",
      "Changing model dtype to torch.float16\n",
      "Model device: cuda:0\n",
      "Tokens shape: torch.Size([120866, 128]), dtype: torch.int64, device: cuda:0\n",
      "Encoder device: cuda:0\n",
      "Encoder device: cuda:0\n",
      "Encoder device: cuda:0\n",
      "Encoder device: cuda:0\n",
      "Encoder device: cuda:0\n",
      "Encoder device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "lin = SAELinearizer(\n",
    "    model_name=\"pythia-70m\",\n",
    "    sae_names=[f\"{x}/0_8192\" for x in [0, 1, 2, 3, 4, 5]],\n",
    "    layers=[0, 1, 2, 3, 4, 5],\n",
    "    dataset_name=\"NeelNanda/pile-10k\",\n",
    "    act_name=\"mlp_out\",\n",
    "    num_batches=50,\n",
    "    run_analysis=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hook_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_rot_q', 'blocks.0.attn.hook_rot_k', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_rot_q', 'blocks.1.attn.hook_rot_k', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_rot_q', 'blocks.2.attn.hook_rot_k', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_rot_q', 'blocks.3.attn.hook_rot_k', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_rot_q', 'blocks.4.attn.hook_rot_k', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_rot_q', 'blocks.5.attn.hook_rot_k', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, cache = lin.model.run_with_cache(lin.data[0])\n",
    "cache.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_freqs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 77.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run_with_cache\n",
      "Looking for blocks.0.hook_mlp_out\n",
      "Cache keys: dict_keys(['blocks.0.hook_mlp_out'])\n",
      "Starting run_with_cache\n",
      "Looking for blocks.0.hook_mlp_out\n",
      "Cache keys: dict_keys(['blocks.0.hook_mlp_out'])\n",
      "Starting run_with_cache\n",
      "Looking for blocks.0.hook_mlp_out\n",
      "Cache keys: dict_keys(['blocks.0.hook_mlp_out'])\n",
      "Starting run_with_cache\n",
      "Looking for blocks.0.hook_mlp_out\n",
      "Cache keys: dict_keys(['blocks.0.hook_mlp_out'])\n",
      "Starting run_with_cache\n",
      "Looking for blocks.0.hook_mlp_out\n",
      "Cache keys: dict_keys(['blocks.0.hook_mlp_out'])\n",
      "Num dead 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.3018, 0.0674, 0.0920,  ..., 0.0238, 0.0465, 0.1105], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from linearization.sae_tutorial import get_freqs\n",
    "\n",
    "get_freqs(\n",
    "    all_tokens=lin.data, model=lin.model, act_name=\"mlp_out\", layer=0, num_batches=5, local_encoder=lin.saes[\"0/0_8192\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: HookedTransformer(\n",
      "  (embed): Embed()\n",
      "  (hook_embed): HookPoint()\n",
      "  (blocks): ModuleList(\n",
      "    (0-5): 6 x TransformerBlock(\n",
      "      (ln1): LayerNormPre(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (ln2): LayerNormPre(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (hook_k): HookPoint()\n",
      "        (hook_q): HookPoint()\n",
      "        (hook_v): HookPoint()\n",
      "        (hook_z): HookPoint()\n",
      "        (hook_attn_scores): HookPoint()\n",
      "        (hook_pattern): HookPoint()\n",
      "        (hook_result): HookPoint()\n",
      "        (hook_rot_k): HookPoint()\n",
      "        (hook_rot_q): HookPoint()\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (hook_pre): HookPoint()\n",
      "        (hook_post): HookPoint()\n",
      "      )\n",
      "      (hook_attn_in): HookPoint()\n",
      "      (hook_q_input): HookPoint()\n",
      "      (hook_k_input): HookPoint()\n",
      "      (hook_v_input): HookPoint()\n",
      "      (hook_mlp_in): HookPoint()\n",
      "      (hook_attn_out): HookPoint()\n",
      "      (hook_mlp_out): HookPoint()\n",
      "      (hook_resid_pre): HookPoint()\n",
      "      (hook_resid_post): HookPoint()\n",
      "    )\n",
      "  )\n",
      "  (ln_final): LayerNormPre(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (unembed): Unembed()\n",
      "), sae: AutoEncoder(), data: tensor([[    0,    68,   306,  ...,    22,   556,  1041],\n",
      "        [    0,  3872,  3988,  ...,  1425,   187,   187],\n",
      "        [    0,   521,  3374,  ...,   253,   346, 49274],\n",
      "        ...,\n",
      "        [    0, 13173, 13645,  ...,   187,    35,   736],\n",
      "        [    0, 10457,   253,  ...,   247,  4229,    14],\n",
      "        [    0,  1892, 47087,  ...,  1142,   273,   841]], device='cuda:0'), layer: 0, num_batches: 5, act_name: mlp_out\n",
      "get_freqs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 46.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run_with_cache\n",
      "Looking for blocks.0.hook_mlp_out\n",
      "Cache keys: dict_keys(['blocks.0.hook_mlp_out'])\n",
      "Starting run_with_cache\n",
      "Looking for blocks.0.hook_mlp_out\n",
      "Cache keys: dict_keys(['blocks.0.hook_mlp_out'])\n",
      "Starting run_with_cache\n",
      "Looking for blocks.0.hook_mlp_out\n",
      "Cache keys: dict_keys(['blocks.0.hook_mlp_out'])\n",
      "Starting run_with_cache\n",
      "Looking for blocks.0.hook_mlp_out\n",
      "Cache keys: dict_keys(['blocks.0.hook_mlp_out'])\n",
      "Starting run_with_cache\n",
      "Looking for blocks.0.hook_mlp_out\n",
      "Cache keys: dict_keys(['blocks.0.hook_mlp_out'])\n",
      "Num dead 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.3061, 0.0666, 0.0930,  ..., 0.0253, 0.0495, 0.1085], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from linearization.analyses.model import frequencies, f1_scores\n",
    "\n",
    "frequencies(model=lin.model, sae=lin.saes[\"0/0_8192\"], data=lin.data, layer=0, num_batches=5, act_name=\"mlp_out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_freqs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 38.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run_with_cache\n",
      "Looking for blocks.0.hook_mlp_out\n",
      "Cache keys: dict_keys(['blocks.0.hook_mlp_out'])\n",
      "Starting run_with_cache\n",
      "Looking for blocks.0.hook_mlp_out\n",
      "Cache keys: dict_keys(['blocks.0.hook_mlp_out'])\n",
      "Starting run_with_cache\n",
      "Looking for blocks.0.hook_mlp_out\n",
      "Cache keys: dict_keys(['blocks.0.hook_mlp_out'])\n",
      "Starting run_with_cache\n",
      "Looking for blocks.0.hook_mlp_out\n",
      "Cache keys: dict_keys(['blocks.0.hook_mlp_out'])\n",
      "Starting run_with_cache\n",
      "Looking for blocks.0.hook_mlp_out\n",
      "Cache keys: dict_keys(['blocks.0.hook_mlp_out'])\n",
      "Num dead 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.3094, 0.0635, 0.0989,  ..., 0.0226, 0.0463, 0.1076], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_freqs(\n",
    "    model=lin.model, local_encoder=lin.saes[\"0/0_8192\"], all_tokens=lin.data, layer=0, num_batches=5, act_name=\"mlp_out\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0/0_8192': AutoEncoder(),\n",
       " '1/0_8192': AutoEncoder(),\n",
       " '2/0_8192': AutoEncoder(),\n",
       " '3/0_8192': AutoEncoder(),\n",
       " '4/0_8192': AutoEncoder(),\n",
       " '5/0_8192': AutoEncoder()}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin.saes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Moving model to device:  cuda\n",
      "Changing model dtype to torch.float16\n",
      "Model device: cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens shape: torch.Size([120866, 128]), dtype: torch.int64, device: cuda:0\n",
      "Encoder device: cuda:0\n",
      "Encoder device: cuda:0\n",
      "Encoder device: cuda:0\n",
      "Encoder device: cuda:0\n",
      "Encoder device: cuda:0\n",
      "Encoder device: cuda:0\n",
      "dict_keys(['0/0_8192', '1/0_8192', '2/0_8192', '3/0_8192', '4/0_8192', '5/0_8192'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 30.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num dead 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 30.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num dead 0.0125732421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 32.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num dead 0.0926513671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 29.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num dead 0.0218505859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 28.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num dead 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 27.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num dead 0.000244140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:54<00:00,  2.29s/it]\n",
      "100%|██████████| 50/50 [00:54<00:00,  1.09s/it]\n",
      "100%|██████████| 50/50 [00:55<00:00,  1.11s/it]\n",
      "100%|██████████| 50/50 [01:33<00:00,  1.88s/it]\n",
      "100%|██████████| 50/50 [01:46<00:00,  2.12s/it]\n",
      "100%|██████████| 50/50 [04:41<00:00,  5.64s/it]\n"
     ]
    }
   ],
   "source": [
    "lin = SAELinearizer(\n",
    "    model_name=\"pythia-70m\",\n",
    "    sae_names=[f\"{x}/0_8192\" for x in [0, 1, 2, 3, 4, 5]],\n",
    "    layers=[0, 1, 2, 3, 4, 5],\n",
    "    dataset_name=\"NeelNanda/pile-10k\",\n",
    "    act_name=\"mlp_out\",\n",
    "    num_batches=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12/25 [00:00<00:00, 34.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.51it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 27.22it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 29.78it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 28.38it/s]\n"
     ]
    }
   ],
   "source": [
    "lin.set_feature(10, \"1/0_8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
