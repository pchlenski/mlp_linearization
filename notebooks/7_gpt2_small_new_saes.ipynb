{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from linearization.analyzer import SAELinearizer\n",
    "from linearization.visualization import visualize_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# SAE_NUM = \"0_8192\"\n",
    "# SAE_NUM = \"1_32768\"\n",
    "# SAE_NUM = \"2_32768\"\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "N_TOP_TOKENS = 15\n",
    "\n",
    "NORMALIZE_SAE_VECS = False\n",
    "# NORMALIZE_SAE_VECS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cuda\n",
      "Changing model dtype to torch.float16\n",
      "Model device: cuda:0\n",
      "Tokens shape: torch.Size([136625, 128]), dtype: torch.int64, device: cuda:0\n",
      "Loading GPT2-small layer ../scripts/checkpoints/64x_resid_mid/layer0/final_sparse_autoencoder_gpt2-small_blocks.0.hook_resid_mid_49152.pt from disk\n",
      "Encoder device: cuda:0\n",
      "dict_keys(['../scripts/checkpoints/64x_resid_mid/layer0/final_sparse_autoencoder_gpt2-small_blocks.0.hook_resid_mid_49152.pt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 13.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num dead 0.6428426504135132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:54<00:00, 17.46s/it]\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class 'linearization.sae_tutorial.AutoEncoder'>: it's not the same object as linearization.sae_tutorial.AutoEncoder",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 21\u001b[0m\n\u001b[1;32m      6\u001b[0m lin \u001b[38;5;241m=\u001b[39m SAELinearizer(\n\u001b[1;32m      7\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2-small\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     sae_names\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# run_analysis=False,\u001b[39;00m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class 'linearization.sae_tutorial.AutoEncoder'>: it's not the same object as linearization.sae_tutorial.AutoEncoder"
     ]
    }
   ],
   "source": [
    "path = \"../data/pickled_linearizers/gpt2_small_resid_mid.pkl\"\n",
    "\n",
    "if os.path.exists(path):\n",
    "    lin = pickle.load(open(path, \"rb\"))\n",
    "else:\n",
    "    lin = SAELinearizer(\n",
    "        model_name=\"gpt2-small\",\n",
    "        sae_names=[\n",
    "            \"../scripts/checkpoints/64x_resid_mid/layer0/final_sparse_autoencoder_gpt2-small_blocks.0.hook_resid_mid_49152.pt\"\n",
    "        ],\n",
    "        layers=[0],\n",
    "        dataset_name=\"NeelNanda/pile-10k\",\n",
    "        act_name=\"resid_mid\",\n",
    "        num_batches=10,\n",
    "        use_gpt=True,\n",
    "        # dict_mult=64,\n",
    "        # hook_point=\"resid_mid\",\n",
    "        # run_analysis=False,\n",
    "    )\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(lin, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cfg', 'state_dict'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sae = torch.load(\n",
    "    \"/home/phil/mlp_linearization/scripts/checkpoints/64x_resid_mid/layer0/final_sparse_autoencoder_gpt2-small_blocks.0.hook_resid_mid_49152.pt\"\n",
    ")\n",
    "sae.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModelSAERunnerConfig(model_name='gpt2-small', hook_point='blocks.0.hook_resid_mid', hook_point_layer=0, hook_point_head_index=None, dataset_path='Skylion007/openwebtext', is_dataset_tokenized=False, context_size=128, use_cached_activations=False, cached_activations_path='activations/Skylion007_openwebtext/gpt2-small/blocks.0.hook_resid_mid', d_in=768, n_batches_in_buffer=128, total_training_tokens=300000000, store_batch_size=32, device=device(type='cuda'), seed=42, dtype=torch.float32, b_dec_init_method='geometric_median', expansion_factor=64, from_pretrained_path=None, l1_coefficient=8e-05, lr=0.0004, lr_scheduler_name='constantwithwarmup', lr_warm_up_steps=5000, train_batch_size=4096, use_ghost_grads=True, feature_sampling_window=1000, feature_sampling_method=None, resample_batches=32, feature_reinit_scale=0.2, dead_feature_window=5000, dead_feature_estimation_method='no_fire', dead_feature_threshold=1e-06, log_to_wandb=False, wandb_project='mats_sae_training_gpt2', wandb_entity=None, wandb_log_frequency=100, n_checkpoints=10, checkpoint_path='checkpoints/h8od9oay')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae[\"cfg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
